/*
 * 2016, 2018 Tarpeeksi Hyvae Soft
 *
 * A simple backpropagating feedforward neural network.
 *
 */

#ifndef NEURAL_NETWORK_H
#define NEURAL_NETWORK_H

#include <vector>
#include <random>
#include <chrono>
#include "../../src/train_on/mnist/mnist_data.h"
#include "../../src/common.h"

// Types of functions we can apply to the sum of the inputs to a neuron to produce its output value.
enum class activation_function_e
{
    none = 0,
    relu,
    leaky_relu,
    log_sigmoid,
    tanh_sigmoid,
    mtanh_sigmoid,
    softmax
};

// The basic element of the neural network; takes a sum of inputs from nodes in the previous layer, and applies a function to that
// sum to produce an output (which may feed into further neurons in the net).
struct neuron_s
{
    // The weights of all inputs to this neuron. Will equal the number of neurons in the
    // previous layer.
    std::vector<real> inputWeights;

    // Weight of the bias connection to this node.
    real biasWeight;

    // The activation value that this neuron sends forward.
    real output;

    // The error delta.
    real delta;

    // Initializes the values of the neuron. Specifically, a number of weights is created to match the number of neurons in the
    // preceding layer, i.e. the number of neurons connecting to this neuron. The weights are given random starting values using
    // the provided random number generator.
    neuron_s(const int precedingLayerSize, std::mt19937 randomNumberGenerator, std::normal_distribution<real> *const randomDistribution)
    {
        output = 0;
        biasWeight = 0;
        delta = 0;

        for (int i = 0; i < precedingLayerSize; i++)
        {
            real randomWeight = randomDistribution->operator()(randomNumberGenerator);

            // Adjust the weight to a range corresponding to the number of output connections to this neuron (as per He et al. 2015).
            randomWeight *= sqrt(2.0 / precedingLayerSize);

            inputWeights.push_back(randomWeight);
        }
    }
};

// Forms the large-scale structure of the neural network by collecting together n number of neurons that share a purpose.
struct neuron_layer_s
{
    // A list of the neurons in this layer.
    std::vector<neuron_s> neurons;

    // The function to apply to the input values of the layer's neurons to produce their output.
    activation_function_e activationFunction = activation_function_e::none;
};

class nnetwork_c
{
public:
    nnetwork_c();
    ~nnetwork_c();

    // Feeds the given input through the neural network and adjusts the weights of the network given any possible mismatch between the
    // expected output and the output generated by the network. Returns the loss function, i.e. an estimate of how 'wrong' the current
    // output of the network is, compared to the expected output.
    real train(const std::vector<real> input, const std::vector<real> expectedOutput);

    // Sends the given input through the neural network. The net's output can then be read from the output neurons.
    void propagate(const std::vector<real> input);

    // Creates a neuron layer of the given number of neurons, and adds it to the neural network. Note that the first layer added via
    // this function will be treated as the input layer, and the last layer added will be treated as the output layer.
    void add_layer(const uint numNeurons, const activation_function_e functionType);

    // Returns a random(-ish) number in the range 0..1.
    real random_number(void);

    // Returns the output value of the given neuron of the output layer.
    real output_of_neuron(const uint outputNeuron) { return real(layers.back().neurons.at(outputNeuron).output); }

    // Returns true if the given output neuron's output value exceeds the activation threshold.
    bool output_neuron_fires(const uint outputNeuron) { return bool(output_of_neuron(outputNeuron) > activationThreshold); }

    // Returns the index in the output layer of the strongest neuron.
    uint strongest_output_neuron_idx(void);

    // Returns a vector where the highest activation for a class is marked by 1 and others as 0.
    std::vector<real> activation_vector(void);

    // For each neuron in the given layer, returns a vector of its input weights.
    std::vector<std::vector<real>> get_weights_in_layer(const uint layer);

    // Run a diagnostic test on the neural network. In this test, the net attempts to learn XOR.
    // The percentage (0-100) of correct trials will be returned. Ideally, the result would be 100%
    // every time.
    static real xor_test(void);

    // Prints to the terminal the net's current configuration, e.g. layer layout etc.
    bool announce_current_configuration() const;

    void set_learning_rate(const real rate) { learningRate = rate; }

    void set_num_training_epochs(const uint epochs) { numTrainingEpochs = epochs; }

    void set_activation_threshold(const real thresh) { activationThreshold = thresh; }

    uint num_training_epochs(void) const;

    uint num_layers(void) const;

private:
    // Send the given input through the neural network to produce output.
    void propagate_forward();

    // In training, calculate the error between the produced output (from forward-propagation) and the output that was expected. Propagate
    // that error from the output neuron(s) to the neurons in preceding layers.
    void propagate_back();

    // Based on the error terms calculated during backpropagation, update the input weights to each neuron.
    void update_weights();

    // Express the difference between the neural network's output and the expected output.
    real loss_function();

    // Takes an array of values and assigns those values to the network's input neurons. Note that the size of this array must
    // match the number of input neurons in the network.
    void set_inputs(const std::vector<real> inputs);

    // Tell the net which output values it should expect to be produced when the next set of inputs is passed along. Used
    // for training.
    void set_expected_output(const std::vector<real> expected);

    // Applies the softmax output function to the output neurons' sums.
    void apply_softmax_to_output();

    // Decides on the type of activation function to call, and returns the output from that activation function given the provided sum.
    inline real activation_function(const real sum, const activation_function_e functionType) const;

    // Calls a derivative function that corresponds to the neuron's activation function, and returns the output of that derivate.
    inline real activation_function_derivative(const real output, const activation_function_e functionType) const;

    // Activation functions.
    //
    // Logistic sigmoid.
    inline real af_logsigmoid(const real x)           const { return (1 / (1 + exp(-x)));                               }
    inline real af_logsigmoid_deriv(const real x)     const { return (x * (1 - x));                                     }
    //
    // Rectified linear unit.
    inline real af_relu(const real x)                 const { return ((x > 0)? x : 0);                                  }
    inline real af_relu_deriv(const real x)           const { return ((x > 0)? 1 : 0);                                  }
    //
    // Leaky rectified linear unit.
    inline real af_leakyrelu(const real x)            const { return ((x > 0)? x : (0.01 * x));                         }
    inline real af_leakyrelu_deriv(const real x)      const { return ((x > 0)? 1 : 0.01);                               }
    //
    // Hyperbolic tangent sigmoid.
    inline real af_tanhsigmoid(const real x)          const { return tanh(x);                                           }
    inline real af_tanhsigmoid_deriv(const real x)    const { return (1 - (x * x));                                     }
    //
    // Modified hyperbolic tangent sigmoid (LeCun et al. 1998).
    inline real af_modtanhsigmoid(const real x)       const { return (1.7159 * tanh(0.6667 * x));                       }
    inline real af_modtanhsigmoid_deriv(const real x) const { return ((0.6667 / 1.7159) * (1.7159 - x) * (1.7159 + x)); }

    // The size of steps the network takes in adjusting its weights. Smaller weights
    // mean slower learning, while larger weights mean less precise learning. Typical
    // values: 0.01 to 0.0001, depending on the dataset.
    real learningRate = 0.01;

    // How many epochs to run when training the net.
    uint numTrainingEpochs = 10;

    // If an output neuron's output value is above this number, we consider the neuron to fire.
    real activationThreshold = 0.5;

    std::vector<neuron_layer_s> layers;

    // The value for each output neuron in the network that we expect the network to produce when next
    // executed on input data.
    std::vector<real> expectedOutput;

    std::mt19937 randomNumberGenerator;

    // This distribution is used to feed random weights into the network (Gaussian with a mean of 0 and a standard deviation of 1).
    std::normal_distribution<real> *const randomNormalDistribution = new std::normal_distribution<real>(0, 1);

    std::uniform_real_distribution<real> *const randomUniformDistribution = new std::uniform_real_distribution<real>(0, 1);
};

#endif
